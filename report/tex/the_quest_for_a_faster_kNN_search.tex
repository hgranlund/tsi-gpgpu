%!TEX root = ./main.tex

\chapter{The quest for a faster kNN search} % (fold)
\label{sec:the_quest_for_a_faster_knn_search}


\section{A short evaluation of OpenCL and CUDA} % (fold)
\label{sub:a_short_evaluation_of_opencl_and_cuda}

% section a_short_evaluation_of_opencl_and_cuda (end)


\section{Investigation of a brute force approach based on Garcia} % (fold)
\label{sub:investigation_of_a_brute_force_approach_based_on_garcia}

% section investigation_of_a_brute_force_approach_based_on_garcia (end)


\section{Application of k-d trees to the kNN problem} % (fold)
\label{sub:application_of_kd_trees_to_the_knn_problem}

A common strategy when wanting to improve the performance of repeated queries in a large dataset, is to organize the dataset into some data structure especially suited for fast querying. This strategy trades the additional time required building an data structure for increased performance on each query. In Section~\ref{sub:investigation_of_a_brute_force_approach_based_on_garcia} we developed an optimized parallel brute force algorithm for performing kNN queries on a large point cloud. In this section we will investigate the possibility of improving on the brute force algorithm by using the k-d tree data structure.

\begin{myrq}
\label{rq:serial-kd-tree}
    It is possible to use a k-d tree, to increase the performance of a large number of repeated, 3-d, low k queries in a large dataset?
\end{myrq}

A brief argument for why k-d trees is well suited for kNN query operations is given, then we will present the k-d tree data structure, and show how it can be used for operating on three-dimensional point cloud data. Finally a set of tests are performed on implementations of the k-d tree based algorithms, in order to determine the possible benefits of a parallel k-d tree based algorithm.

\subsection{Why k-d trees?} % (fold)
\label{sub:why_k_d_trees_}
A large part of this thesis is devoted to applying k-d trees to the kNN problem. The reader might ask themselves why this is so. Other possible data structures exist which is optimized for querying in geometrical data. Why choose to investigate k-d trees in particular?

Part of the explanation has to do with the scope and time resources available for the work in this thesis. Performing a full analysis and parallelization of every possible data structure, and their associated query algorithms, would just not be feasible within our time frame. That said, k-d trees is a very attractive data structure for our use case.

\begin{itemize}
    \item k-d trees are easy to understand and implement, leaving more time to throughly investigate parallelization of the algorithms.
    \item k-d trees are a very minimal data structure, and balanced k-d trees are complete binary trees. This makes reducing the amount of additional memory required in addition to the 3-d points a relative simple task, something that is important, considering the memory bounds on GPUs, and the time penalty associated with moving data from system memory to GPU memory.
    \item k-d trees are well adapted to performing associative queries, where the query is for a point that is not equal to, but close to the query point.
    \item Studies on parallel kNN queries based on k-d trees has been documented in literature with encouraging results\cite{Owens:2007:ASO}. %Flere papers som passer inn her?
    %Flere punkter?
\end{itemize}

%Mulig å slenge på en rask gjennomgang av alternativer under her, men er litt usikker på om det blir ganske langt og fjerner fokus litt mye fra k-d.

% subsection why_k_d_trees_ (end)

\subsection{Building k-d trees for point cloud data} % (fold)
\label{ssub:building_k_d_trees_for_point_cloud_data}

A k-d tree can be thought of as a binary search tree in k dimensions. A binary search tree is constructed such that, for a given node, one of Its child-subtrees is consisting of elements smaller than the current node, and the other child-subtree is consisting of elements larger than the current node. The same strategy is applied when constructing a k-d tree, but at each level we are sorting the child-subtree elements according to one selected dimension, called the discriminant for this level. This discriminant is cycled through the different dimensions, as we move down each level in the tree. A formal description of k-d trees is given by Jon Louis Bentley in the paper Multidimensional Binary Search Trees Used for Associative Searching\cite{Bentley:1975:MBS:361002.361007}.

Let us have a look at an example using data for two dimensions. Figure~\ref{fig:kd_tree_2d_plane} shows us a set of points on a two dimensional plane. The lines through each point indicate the split plane formed by the discriminant associated with the different points.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=120mm]{../gfx/kd_tree_illustration_graph.png}
    \caption{A set of points on a plane, with a possible k-d tree indicated.}
    \label{fig:kd_tree_2d_plane}
\end{figure}

The corresponding k-d tree is shown in Figure~\ref{fig:kd_tree_2d}. Note that lower values in each level are placed in the left branches, and higher values are placed in the right branches.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=120mm]{../gfx/kd_tree_illustration_tree.png}
    \caption{Tree representation of the points in Figure~\ref{fig:kd_tree_2d_plane}.}
    \label{fig:kd_tree_2d}
\end{figure}

By extending this example with three fixed dimensions for the spatial dimensions, x, y, and z, we get a k-d tree suitable for storing point cloud data.

It it possible to construct several algorithms for building k-d trees from a set of points, and one simple approach is using a recursive function. Algorithm \ref{alg:seriel_tree_build} shows pseudocode for such a simple tree building algorithm. In the pseudocode, we have chosen to represent the different dimensions as a natural number. This means that x is represented by 0, y is represented by 1, z is represented by 2 and so on. Given a set of point, $P$, in $k$ space, and a initial split dimension $i$, it constructs a balanced k-d tree.

\begin{algorithm}
\caption{Recursive k-d tree build}
\label{alg:seriel_tree_build}
\begin{algorithmic}
    \Function{Build-KD-Tree}{$P$, $i$}
        \If{$P.length = 0$} \Comment{We have reached the end of a branch}
            \State \textbf{return} NIL
        \Else
            \State $m \gets \text{Median}(P)$

            \State \text{Let $L$ be all elements of $P < m$ in dimension $i$}
            \State \text{Let $H$ be all elements of $P > m$ in dimension $i$}

            \State $i' \gets (i + 1) \bmod k$ \Comment{k = 3 for a three dimensional k-d tree}

            \State $m.left \gets \text{Build-KD-Tree}(L, i')$
            \State $m.right \gets \text{Build-KD-Tree}(H, i')$
        \EndIf
        \State \textbf{return} $m$
    \EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:seriel_tree_build} starts by checking if there is any more points left in $P$. If not, it returns NIL as an end of branch marker. If there still is points left, the algorithm selects the median point, $m$, as the root node. Then it sorts all remaining points into a collection of points lower than the median, $L$, and higher than the median, $H$. The dimension, $i$, is incremented, and the Build-KD-Tree function is called recursively on both collections of points. Finally the root node is returned, so it can be assigned as the child of its parent node, or be used as a global root node.

It is worth to note that the performance of this k-d tree build algorithm is sensitive to the choice of a median finding algorithm, since we will be querying for the median \BigO{n} times. Choosing to just sorting the collection $P$, and selecting the median from the middle of the sorted collection, will not give optimal results. Fortunately, several \BigO{n} median selecting algorithms exist\cite{Cormen:2001} (Get chapter citation), quickselect, being the choice for our initial implementations. This gives a algorithm with a time complexity of \BigO{k*n*log(n)}\cite{Friedman:1977}.

A final note about Algorithm~\ref{alg:seriel_tree_build}, is that it does not handles points with duplicate values in one dimension. If the algorithm where to be feed with a point collection where all points had the same value for x, it would not be able to handle it, since such a point does not explicitly belong in $L$ or $H$. Several modifications can be made to handle this case. We can choose to place all conflicting median points, exept one, in either $L$ or $H$. The problem with this solution, is that we are not guaranteed to get a balance tree. If we where to have a set of points, where all points where tha same, we would get a tree at all, but just one long branch of length n. Another strategy is to try to place the conflicting medians, equally in $L$ and $H$. This way the median we select will be the midmost element in the point collection, retaining the balance in the finished k-d tree. Given that we consider that duplicate median points can be located in both subtrees of a node, this will not affect search operations on the tree, as we will see later.
% subsection building_k_d_trees_for_point_cloud_data (end)

\subsection{Querying the k-d tree} % (fold)
\label{sub:querying_the_k_d_tree}

With a k-d tree we can perform efficient searches for the closest point to a given point in \BigO{n*log(n)} average time\cite{Friedman:1977}. By maintaining a collection of the k closest points during execution of the query, we can even perform kNN searches. An example of a kNN search algorithm is shown in Algorithm~\ref{alg:recursive_knn_kd_tree_search}.

(Something about how searching works in the general sense?)

The procedure will take the root of a k-d tree, $r$, a query point, for which we want to find the k closest points, an initial dimension, $i$, which should be the same as the one used to build the tree. It uses this data to manipulate a collection of the k closest points to $q$, stored in $K$.

In our example $K$ is a data structure with some special properties, called the k-heap. You can query it for the maximum distance value of the k points stored in it, and it will only store a predetermined number of points. If you try to insert more points than the predetermined number of points, it will discard the highest values, and only keep the k lowest values. This data structure can be easily implemented as a modified max-heap. When the size of the heap is lower than k, it it used in the usual manner, but when the heap is of size k, a slight modification to the insertion operation is made. Instead of adding the new element to the heap, the new element is swapped with the maximum value of the heap, if it is lower than this maximum value. Then the heap is re-balanced using standard algorithms. In our code, we assume the k-heap to be filled at the start with k points of either a random sample of points from the k-d tree, or with positive infinity. This way we do not need to take into account if the stack is filled yet, during the recursive execution of the procedure.

\begin{algorithm}
\caption{Recursive kNN k-d tree search}
\label{alg:recursive_knn_kd_tree_search}
\begin{algorithmic}
    \Procedure{kNN-KD-Tree}{$K, r, q, i$}
        \If{$r =$ NIL} \Comment{We have reached the end of a branch}
            \State \textbf{return}
        \EndIf

        \State $d \gets \text{Distance}(r, q)$
        \State $dx \gets r.x[i] - q.x[i]$

        \If{$d < K.max$} \Comment{Is $r$ closer to $q$ than the current k best points?}
            \State $r.distance \gets d$
            \State \text{Insert}($K, r$)
        \EndIf

        \State $i' \gets (i + 1) \bmod k$ \Comment{k = 3 for a three dimensional k-d tree}

        \If{$dx > 0$}  \Comment{Select $t$ and $o$ so we traverse towards closest point first}
            \State $t \gets r.left$, $o \gets r.right$
        \Else
            \State $t \gets r.right$, $o \gets r.left$
        \EndIf

        \State \text{kNN-KD-Tree}($K, t, q, i'$)

        \If{$dx^2 < K.max$} \Comment{Can there be closer points in the other subtree?}
            \State \text{kNN-KD-Tree}($K, o, q, i'$)
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:recursive_knn_kd_tree_search} starts by checking if we have reached the end of a branch. If not, it calculates the Euclidean distance between the query point, $q$, and the current root point, $r$. Calculating this distance is a costly step, since it usually involves calculating a square root. This can be circumvented when implementing, by relying on using the square of the Euclidean distance as the distance metric, instead of the actual distance. This will not make a difference for the algorithm. The distance, $dx$, between the current root and the query point in dimension $i$ is also calculated.

The algorithm then checks if the current root point is closer to the query point than one of the points in the k-heap. If this is the case, it inserts the current root into the k-heap. The next dimension, $i'$, is calculated, and then the algorithm determines if it should traverse to the right or left child node first. For efficient querying, we want to traverse down the branch that would contain the query point. In other words, if the query point is lower than the current root point in the current dimension, we want to traverse to the left child, and vice versa. The child node that we want to traverse first, is often called the target, and its corresponding subtree is often called the target subtree. In the algorithm the symbol $t$ is used to represent target. The child and child-subtree that is not chosen for immediate traversal is called other and other-subtree. In the algorithm the symbol $o$ is used to represent other. The ability to prune away the other subtree, given our current best estimates stored in the k-heap and the distance $dx$, is what makes the k-d tree efficient for kNN searches.

After recursively investigating the target subtree, we ask if our estimates in the k-heap is better than the distance $dx$, remembering that the distances stored in the k-heap is squared. If this is the case, we know that there cannot be a closer point in the other subtree, and we can prune it from our search. If not, we have to check the other subtree as well. When the procedure terminated, the k closest points to the query point is stored in the k-heap.
% subsection querying_the_k_d_tree (end)

\subsection{Testing a serial k-d tree based kNN solver} % (fold)
\label{sub:testing_a_serial_k_d_tree_based_knn_solver}

In order to gain some real world insight into the performance characteristics of k-d tree building and querying, a serial implementation of the build and query algorithm was made. This implementation is available in Appendix X. These two implementations where then subjected to several tests, using test setup Y. All tests were performed on a set of randomly generated points 3-d points, with the number of points ranging from $10^5$ to $1.41*10^7$. The raw data from these tests are included in Appendix X. The result of these test are summed up in the following figures.

Figure~\ref{fig:serial-build} shows the timing results for the recursive k-d tree build algorithm.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=120mm]{../gfx/serial-build.png}
    \caption{Timing results for recursive k-d tree building}
    \label{fig:serial-build}
\end{figure}

We observe that constructing a k-d tree for a large number of points is a costly operation. Given a tree of size $1.41^7$ the algorithm uses nearly $9$ seconds to construct the tree. We also note that the timing results seem to scale linearly in relation to the number of points. This relates nicely to calculated time complexity of the algorithm.

Figure \ref{fig:serial-query} shows the timing results for querying a k-d tree of a given size. The k-d tree is queried for one point with $k=1$. Since we are interested in investigating the average performance, $10^5$ consecutive queries was timed, and the average value for one query was calculated.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=120mm]{../gfx/serial-query.png}
    \caption{Timing results for mean query time with k equal to one}
    \label{fig:serial-query}
\end{figure}

We see that querying the k-d tree is very fast on average. Querying for one point in a tree of size $1.41^7$ takes about $0.0014$ milliseconds. It has to be taken into account though, that a query with $k=1$ will give the best query time, since the time complexity of the query algorithm scales with k. Still, for queries with a low $k$, we should expect good performance. The graph also seems to scale with the logarithm of the number of points, as expected by the time complexity calculation.

In order to try to answer RQ~\ref{rq:serial-kd-tree}, we compare the timing results gained from the fastest brute force algorithm developed in Section~\ref{sub:investigation_of_a_brute_force_approach_based_on_garcia}. Figure~\ref{fig:brute-force-vs-serial-build-query} compare the average time required for building a k-d tree of a given size, and performing a single $k=1$ query, to the time required to compute the same result with the fastest brute force algorithm obtained in Section~\ref{sub:investigation_of_a_brute_force_approach_based_on_garcia}.

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{../gfx/brute-force-vs-serial-build-query.png}
\caption{Comparison of mean query time with k equal to one with fast brute force and recursive k-d tree based algorithms}
\label{fig:brute-force-vs-serial-build-query}
\end{figure}

In this comparison, the k-d tree based algorithm does not seem like a good option. When performing just one query, the additional time required to build the k-d tree heavily outweighs the benefit of the improved query time, compared to the brute force solution. This result is to be expected, since we are not really utilizing the benefit of the k-d tree, but it is still an important point that a brute force algorithm can be very efficient for certain use-cases.

Let us finally look at some results more closely related to the use-case given by TSI\@. Figure~\ref{fig:brute-force-vs-serial-build-n-queries} does the same comparison as Figure~\ref{fig:brute-force-vs-serial-build-query}, but instead of comparing the time taken to perform one query, $n$ repeated queries are performed, with $n$ being the size of the k-d tree.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=120mm]{../gfx/brute-force-vs-serial-build-n-queries.png}
    \caption{Comparison of timing of n queries with k equal to one with fast brute force and recursive k-d tree based algorithms}
    \label{fig:brute-force-vs-serial-build-n-queries}
\end{figure}

We observe that in this use-case, the k-d tree based approach have much better results than the brute force based approach. Now the k-d tree only have to be built once, but we benefit from the decreased query time in all $n$ queries. Performing $n$ queries on a point cloud of size $1.41^7$ with the brute force based algorithm takes about $9.7*10^5$ seconds, or about 11 days. With the recursive k-d based algorithm, the same operation can be calculated in just over a minute. Considering the needs of TSI, it seem that this approach is worth developing further into an parallel algorithm.

Despite these initial positive results, some problems are apparent from our initial tests.

The k-d tree building algorithm is very slow. Given that we want to perform kNN queries on larger point clouds than $1.41^7$, finding an efficient parallelization of this algorithm would be very beneficial. This is not as trivial as it might seem, as tree-based algorithms do not lend themselves very well to trivial parallelization.

When scaling the number of repeated queries from one to $n$ we observed the huge impact a seemingly small change in the time required for performing one query had on the time needed to compute the entire result. A change from several milliseconds to a fraction of a milliseconds might seem trivial, but given enough repeated queries, this was the difference between minutes and days of computation time. Will we be able to keep the query time down when increasing the value of $k$, and moving the computation over to the GPU, which generally has a slower clock cycle than the CPU?

Finally all our k-d tree algorithm are based on recursion, which is not usually a good choice for handling large volumes of data on the GPU.

In the next sections we will address these challenges, along with others, and develop a parallel algorithm for performing kNN queries based on k-d trees.
% subsection testing_a_serial_k_d_tree_based_knn_solver (end)
% % section kd_tree_based_effort (end)

\section{Development of a parallel k-d tree algorithm} % (fold)
\label{sec:development_of_a_parallel_k_d_tree_algorithm}

\input{development_of_a_parallel_kd_tree_build_algorithm}
% section development_of_a_parallel_k_d_tree_algorithm (end)

\section{Development of a parallel k-d search algorithm} % (fold)
\label{sec:development_of_a_parallel_k_d_search_algorithm}

\input{development_of_a_parallel_kd_search_algorithm}

% section development_of_a_parallel_k_d_search_algorithm (end)

\section*{Cuda Optimizations}

\begin{enumerate}
\item 32 threads in a warp
\item Number of threads and blocks. Calaculate on each lounch
\item divergence
\item memory placement
\item
\end{enumerate}

\cleardoublepage
